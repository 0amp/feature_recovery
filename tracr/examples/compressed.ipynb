{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns \n",
    "import haiku as hk\n",
    "\n",
    "# The default of float16 can lead to discrepancies between outputs of\n",
    "# the compiled model and the RASP program.\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "from tracr.rasp import rasp\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define RASP programs\n",
    "def get_program(program_name, max_seq_len):\n",
    "  \"\"\"Returns RASP program and corresponding token vocabulary.\"\"\"\n",
    "  if program_name == \"length\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_length()\n",
    "  elif program_name == \"frac_prevs\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"x\"}\n",
    "    program = lib.make_frac_prevs((rasp.tokens == \"x\").named(\"is_x\"))\n",
    "  elif program_name == \"dyck-2\":\n",
    "    vocab = {\"(\", \")\", \"{\", \"}\"}\n",
    "    program = lib.make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n",
    "  elif program_name == \"dyck-3\":\n",
    "    vocab = {\"(\", \")\", \"{\", \"}\", \"[\", \"]\"}\n",
    "    program = lib.make_shuffle_dyck(pairs=[\"()\", \"{}\", \"[]\"])\n",
    "  elif program_name == \"sort\":\n",
    "    vocab = {i for i in range(1, max_seq_len + 1)}\n",
    "    program = lib.make_sort(\n",
    "        rasp.tokens, rasp.tokens, max_seq_len=max_seq_len, min_key=1)\n",
    "  elif program_name == \"sort_unique\":\n",
    "    vocab = {i for i in range(1, max_seq_len + 1)}\n",
    "    program = lib.make_sort_unique(rasp.tokens, rasp.tokens)\n",
    "  elif program_name == \"hist\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_hist()\n",
    "  elif program_name == \"sort_freq\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_sort_freq(max_seq_len=max_seq_len)\n",
    "  elif program_name == \"pair_balance\":\n",
    "    vocab = {\"(\", \")\"}\n",
    "    program = lib.make_pair_balance(\n",
    "        sop=rasp.tokens, open_token=\"(\", close_token=\")\")\n",
    "  else:\n",
    "    raise NotImplementedError(f\"Program {program_name} not implemented.\")\n",
    "  return program, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "   Program: sort_unique\n",
      "   Input vocabulary: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "   Context size: 10\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#@title: Assemble model\n",
    "program_name = \"sort_unique\"  #@param [\"length\", \"frac_prevs\", \"dyck-2\", \"dyck-3\", \"sort\", \"sort_unique\", \"hist\", \"sort_freq\", \"pair_balance\"]\n",
    "max_seq_len = 10 #@param {label: \"Test\", type: \"integer\"}\n",
    "\n",
    "program, vocab = get_program(program_name=program_name,\n",
    "                             max_seq_len=max_seq_len)\n",
    "\n",
    "print(f\"Compiling...\")\n",
    "print(f\"   Program: {program_name}\")\n",
    "print(f\"   Input vocabulary: {vocab}\")\n",
    "print(f\"   Context size: {max_seq_len}\")\n",
    "\n",
    "from tracr.datasets.generated_lib import program_1\n",
    "assembled_model = compiling.compile_rasp_to_model(\n",
    "      program=program,\n",
    "      vocab=vocab,\n",
    "      max_seq_len=max_seq_len,\n",
    "      causal=False,\n",
    "      use_dropout=False, \n",
    "      embedding_size=30,\n",
    "      unembed_at_every_layer=True,\n",
    "      compiler_bos=\"bos\",\n",
    "      compiler_pad=\"pad\",\n",
    "      mlp_exactness=100)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# dict_keys(['token_embed', 'pos_embed', 'transformer/layer_0/attn/query', 'transformer/layer_0/attn/key', 'transformer/layer_0/attn/value', 'transformer/layer_0/attn/linear', 'transformer/layer_0/mlp/linear_1', 'transformer/layer_0/mlp/linear_2', 'transformer/layer_1/attn/query', 'transformer/layer_1/attn/key', 'transformer/layer_1/attn/value', 'transformer/layer_1/attn/linear', 'transformer/layer_1/mlp/linear_1', 'transformer/layer_1/mlp/linear_2'])\n",
    "# dict_keys(['token_embed', 'pos_embed', 'compressed_transformer/layer_0/attn/query', 'compressed_transformer/layer_0/attn/key', 'compressed_transformer/layer_0/attn/value', 'compressed_transformer/layer_0/attn/linear', 'compressed_transformer/layer_0/mlp/linear_1', 'compressed_transformer/layer_0/mlp/linear_2', 'compressed_transformer/layer_1/attn/query', 'compressed_transformer/layer_1/attn/key', 'compressed_transformer/layer_1/attn/value', 'compressed_transformer/layer_1/attn/linear', 'compressed_transformer/layer_1/mlp/linear_1', 'compressed_transformer/layer_1/mlp/linear_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Forward pass\n",
    "assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).decoded\n",
    "# assembled_model.apply(['bos', 'a', 'b', 'c', 'x']).decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_embed', 'pos_embed', 'compressed_transformer', 'compressed_transformer/layer_0/attn/query', 'compressed_transformer/layer_0/attn/key', 'compressed_transformer/layer_0/attn/value', 'compressed_transformer/layer_0/attn/linear', 'compressed_transformer/layer_0/mlp/linear_1', 'compressed_transformer/layer_0/mlp/linear_2', 'compressed_transformer/layer_1/attn/query', 'compressed_transformer/layer_1/attn/key', 'compressed_transformer/layer_1/attn/value', 'compressed_transformer/layer_1/attn/linear', 'compressed_transformer/layer_1/mlp/linear_1', 'compressed_transformer/layer_1/mlp/linear_2'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compressed_transformer': {'w_emb': (30, 45)},\n",
       " 'compressed_transformer/layer_0/attn/key': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/attn/linear': {'b': (45,), 'w': (12, 45)},\n",
       " 'compressed_transformer/layer_0/attn/query': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/attn/value': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/mlp/linear_1': {'b': (22,), 'w': (45, 22)},\n",
       " 'compressed_transformer/layer_0/mlp/linear_2': {'b': (45,), 'w': (22, 45)},\n",
       " 'compressed_transformer/layer_1/attn/key': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/attn/linear': {'b': (45,), 'w': (12, 45)},\n",
       " 'compressed_transformer/layer_1/attn/query': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/attn/value': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/mlp/linear_1': {'b': (22,), 'w': (45, 22)},\n",
       " 'compressed_transformer/layer_1/mlp/linear_2': {'b': (45,), 'w': (22, 45)},\n",
       " 'pos_embed': {'embeddings': (11, 45)},\n",
       " 'token_embed': {'embeddings': (12, 45)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "@hk.transform\n",
    "def forward_fn(inputs): \n",
    "    compiled_model = assembled_model.get_compiled_model()\n",
    "    return compiled_model(inputs)\n",
    "\n",
    "dummy = jnp.zeros((1, 10), dtype=jnp.int32)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = forward_fn.init(rng, dummy)\n",
    "print(params.keys())\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1951/3075794209.py:26: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  assert num_samples <= np.math.factorial(max_len)\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 10\n",
    "vocab = [i for i in range(1, max_seq_len + 1)]\n",
    "program_name = 'sort_unique'\n",
    "\n",
    "def get_permutations(vocab, rem_len, no_repeats=False):\n",
    "    # base case\n",
    "    if rem_len == 1: \n",
    "        return [[v] for v in vocab]\n",
    "        \n",
    "    res = []\n",
    "    for v in vocab:\n",
    "        if no_repeats:\n",
    "            vocab = [v_ for v_ in vocab if v_ != v]\n",
    "        perms = get_permutations(vocab, rem_len - 1)\n",
    "        for p in perms: \n",
    "            p.append(v)\n",
    "        res.extend(perms)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_permutations_via_sampling(vocab, max_len, num_samples, no_repeats=False): \n",
    "    \"\"\"\n",
    "    Works iteratively, want to sample uniformally from all permutations of length rem_len\n",
    "    \"\"\"\n",
    "    if no_repeats: \n",
    "        assert num_samples <= np.math.factorial(max_len)\n",
    "    else: \n",
    "        assert num_samples <= len(vocab) ** max_len\n",
    "        \n",
    "    permutations = []\n",
    "    for _ in range(num_samples): \n",
    "        if no_repeats: \n",
    "            permutation = np.random.choice(vocab, size=max_len, replace=False)\n",
    "        else:\n",
    "            permutation = np.random.choice(vocab, size=max_len, replace=True)\n",
    "        permutations.append(permutation)\n",
    "    return permutations\n",
    "        \n",
    "\n",
    "dataset = get_permutations_via_sampling(vocab, max_seq_len, 1000, no_repeats=True)\n",
    "expected = [['bos'] + sorted(d) for d in dataset]\n",
    "dataset = [['bos'] + list(d) for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bos', 6, 3, 9, 8, 5, 2, 1, 7, 10, 4] ['bos', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0], expected[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "   Program: sort_unique\n",
      "   Input vocabulary: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "   Context size: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#@title: Assemble model\n",
    "program_name = \"sort_unique\"  #@param [\"length\", \"frac_prevs\", \"dyck-2\", \"dyck-3\", \"sort\", \"sort_unique\", \"hist\", \"sort_freq\", \"pair_balance\"]\n",
    "max_seq_len = 10 #@param {label: \"Test\", type: \"integer\"}\n",
    "\n",
    "program, vocab = get_program(program_name=program_name,\n",
    "                             max_seq_len=max_seq_len)\n",
    "\n",
    "print(f\"Compiling...\")\n",
    "print(f\"   Program: {program_name}\")\n",
    "print(f\"   Input vocabulary: {vocab}\")\n",
    "print(f\"   Context size: {max_seq_len}\")\n",
    "\n",
    "from tracr.datasets.generated_lib import program_1\n",
    "assembled_model = compiling.compile_rasp_to_model(\n",
    "      program=program,\n",
    "      vocab=vocab,\n",
    "      max_seq_len=max_seq_len,\n",
    "      causal=False,\n",
    "      use_dropout=False, \n",
    "      embedding_size=None,\n",
    "      unembed_at_every_layer=False,\n",
    "      compiler_bos=\"bos\",\n",
    "      compiler_pad=\"pad\",\n",
    "      mlp_exactness=100)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# dict_keys(['token_embed', 'pos_embed', 'transformer/layer_0/attn/query', 'transformer/layer_0/attn/key', 'transformer/layer_0/attn/value', 'transformer/layer_0/attn/linear', 'transformer/layer_0/mlp/linear_1', 'transformer/layer_0/mlp/linear_2', 'transformer/layer_1/attn/query', 'transformer/layer_1/attn/key', 'transformer/layer_1/attn/value', 'transformer/layer_1/attn/linear', 'transformer/layer_1/mlp/linear_1', 'transformer/layer_1/mlp/linear_2'])\n",
    "# dict_keys(['token_embed', 'pos_embed', 'compressed_transformer/layer_0/attn/query', 'compressed_transformer/layer_0/attn/key', 'compressed_transformer/layer_0/attn/value', 'compressed_transformer/layer_0/attn/linear', 'compressed_transformer/layer_0/mlp/linear_1', 'compressed_transformer/layer_0/mlp/linear_2', 'compressed_transformer/layer_1/attn/query', 'compressed_transformer/layer_1/attn/key', 'compressed_transformer/layer_1/attn/value', 'compressed_transformer/layer_1/attn/linear', 'compressed_transformer/layer_1/mlp/linear_1', 'compressed_transformer/layer_1/mlp/linear_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m token_embed \u001b[38;5;241m=\u001b[39m \u001b[43mparams\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embed\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m w_emb \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompressed_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_emb\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(w_emb\u001b[38;5;241m.\u001b[39mshape, token_embed\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "token_embed = params['token_embed']['embeddings']\n",
    "w_emb = params['compressed_transformer']['w_emb']\n",
    "print(w_emb.shape, token_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m einsum\n\u001b[0;32m----> 3\u001b[0m last_out \u001b[38;5;241m=\u001b[39m \u001b[43massembled_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransformer_output\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# last_out = einsum(assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).residuals[-1], w_emb, 'b s c, c d -> b s d')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m logits \u001b[38;5;241m=\u001b[39m einsum(last_out, token_embed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb s d, v d -> b s v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/compiler/assemble.py:74\u001b[0m, in \u001b[0;36mAssembledTransformerModel.apply\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m decoded \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39munembedded_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_encoder:\n\u001b[0;32m---> 74\u001b[0m   decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_encoder\u001b[38;5;241m.\u001b[39mbos_token:\n\u001b[1;32m     77\u001b[0m   \u001b[38;5;66;03m# Special case for decoding the bos token position, for which the output\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   \u001b[38;5;66;03m# decoder might have unspecified behavior.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m   decoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_encoder\u001b[38;5;241m.\u001b[39mbos_token] \u001b[38;5;241m+\u001b[39m decoded[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/transformer/encoder.py:112\u001b[0m, in \u001b[0;36mCategoricalEncoder.decode\u001b[0;34m(self, encodings)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recover the tokens that corresponds to `ids`. Inverse of __call__.\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m decoding_map \u001b[38;5;241m=\u001b[39m {val: key \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(decoding_map\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in decoding map \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m                    decoding_map\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [decoding_map[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m encodings]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from einops import einsum\n",
    "\n",
    "last_out = assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).transformer_output\n",
    "# last_out = einsum(assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).residuals[-1], w_emb, 'b s c, c d -> b s d')\n",
    "logits = einsum(last_out, token_embed, 'b s d, v d -> b s v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[10,  2,  3,  0,  6,  1,  5,  4,  7,  8,  9]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massembled_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munembedded\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/compiler/assemble.py:74\u001b[0m, in \u001b[0;36mAssembledTransformerModel.apply\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     72\u001b[0m decoded \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39munembedded_output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_encoder:\n\u001b[0;32m---> 74\u001b[0m   decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_encoder\u001b[38;5;241m.\u001b[39mbos_token:\n\u001b[1;32m     77\u001b[0m   \u001b[38;5;66;03m# Special case for decoding the bos token position, for which the output\u001b[39;00m\n\u001b[1;32m     78\u001b[0m   \u001b[38;5;66;03m# decoder might have unspecified behavior.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m   decoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_encoder\u001b[38;5;241m.\u001b[39mbos_token] \u001b[38;5;241m+\u001b[39m decoded[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/transformer/encoder.py:112\u001b[0m, in \u001b[0;36mCategoricalEncoder.decode\u001b[0;34m(self, encodings)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Recover the tokens that corresponds to `ids`. Inverse of __call__.\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m decoding_map \u001b[38;5;241m=\u001b[39m {val: key \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(decoding_map\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in decoding map \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m                    decoding_map\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [decoding_map[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m encodings]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).unembedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `hk.Module`s must be initialized inside an `hk.transform`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43massembled_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_compiled_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/compiler/assemble.py:253\u001b[0m, in \u001b[0;36massemble_craft_model.<locals>.get_compiled_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_compiled_model\u001b[39m():\n\u001b[0;32m--> 253\u001b[0m   embed_modules \u001b[38;5;241m=\u001b[39m \u001b[43m_make_embedding_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mresidual_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresidual_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtokens_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindices_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m   transformer \u001b[38;5;241m=\u001b[39m compressed_model\u001b[38;5;241m.\u001b[39mCompressedTransformer(model_config)\n\u001b[1;32m    259\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mCompiledTransformerModel(\n\u001b[1;32m    260\u001b[0m     transformer\u001b[38;5;241m=\u001b[39mtransformer,\n\u001b[1;32m    261\u001b[0m     token_embed\u001b[38;5;241m=\u001b[39membed_modules\u001b[38;5;241m.\u001b[39mtoken_embed,\n\u001b[1;32m    262\u001b[0m     position_embed\u001b[38;5;241m=\u001b[39membed_modules\u001b[38;5;241m.\u001b[39mpos_embed,\n\u001b[1;32m    263\u001b[0m     unembed\u001b[38;5;241m=\u001b[39membed_modules\u001b[38;5;241m.\u001b[39munembed,\n\u001b[1;32m    264\u001b[0m     use_unembed_argmax\u001b[38;5;241m=\u001b[39mcategorical_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/tracr/compiler/assemble.py:186\u001b[0m, in \u001b[0;36m_make_embedding_modules\u001b[0;34m(residual_space, tokens_space, indices_space, output_space)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Token embeddings.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m res_to_out \u001b[38;5;241m=\u001b[39m vectorspace_fns\u001b[38;5;241m.\u001b[39mproject(residual_space, output_space)\n\u001b[0;32m--> 186\u001b[0m token_embed \u001b[38;5;241m=\u001b[39m \u001b[43mhk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pytype: disable=wrong-arg-types  # jax-ndarray\u001b[39;49;00m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_to_res\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Positional embeddings.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m index_to_res \u001b[38;5;241m=\u001b[39m vectorspace_fns\u001b[38;5;241m.\u001b[39mproject(indices_space, residual_space)\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/haiku/_src/module.py:139\u001b[0m, in \u001b[0;36mModuleMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Now attempt to initialize the object.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m init \u001b[38;5;241m=\u001b[39m wrap_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (config\u001b[38;5;241m.\u001b[39mget_config()\u001b[38;5;241m.\u001b[39mmodule_auto_repr \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTO_REPR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m    143\u001b[0m   module\u001b[38;5;241m.\u001b[39m_auto_repr \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mauto_repr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fr/lib/python3.10/site-packages/haiku/_src/module.py:433\u001b[0m, in \u001b[0;36mwrap_method.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the original method with a group name set before and after.\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m base\u001b[38;5;241m.\u001b[39mframe_stack:\n\u001b[0;32m--> 433\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `hk.Module`s must be initialized inside an `hk.transform`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Submodules are associated with this method. We allow users to associate\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# submodules with a different method than the one being called via\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# `@name_like(\"other_method\")`. Interceptors and custom getters are still\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# provided the actual method name (e.g. \"submodule_method_name\" is only used\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# for naming submodules).\u001b[39;00m\n\u001b[1;32m    441\u001b[0m submodule_method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(unbound_method, _CUSTOM_NAME, method_name)\n",
      "\u001b[0;31mValueError\u001b[0m: All `hk.Module`s must be initialized inside an `hk.transform`."
     ]
    }
   ],
   "source": [
    "assembled_model.get_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def get_logits(inputs): \n",
    "    compiled_model = assembled_model.get_compiled_model()\n",
    "    return compiled_model(inputs, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
