{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns \n",
    "import haiku as hk\n",
    "\n",
    "# The default of float16 can lead to discrepancies between outputs of\n",
    "# the compiled model and the RASP program.\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "from tracr.rasp import rasp\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define RASP programs\n",
    "def get_program(program_name, max_seq_len):\n",
    "  \"\"\"Returns RASP program and corresponding token vocabulary.\"\"\"\n",
    "  if program_name == \"length\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_length()\n",
    "  elif program_name == \"frac_prevs\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"x\"}\n",
    "    program = lib.make_frac_prevs((rasp.tokens == \"x\").named(\"is_x\"))\n",
    "  elif program_name == \"dyck-2\":\n",
    "    vocab = {\"(\", \")\", \"{\", \"}\"}\n",
    "    program = lib.make_shuffle_dyck(pairs=[\"()\", \"{}\"])\n",
    "  elif program_name == \"dyck-3\":\n",
    "    vocab = {\"(\", \")\", \"{\", \"}\", \"[\", \"]\"}\n",
    "    program = lib.make_shuffle_dyck(pairs=[\"()\", \"{}\", \"[]\"])\n",
    "  elif program_name == \"sort\":\n",
    "    vocab = {i for i in range(1, max_seq_len + 1)}\n",
    "    program = lib.make_sort(\n",
    "        rasp.tokens, rasp.tokens, max_seq_len=max_seq_len, min_key=1)\n",
    "  elif program_name == \"sort_unique\":\n",
    "    vocab = {i for i in range(1, max_seq_len + 1)}\n",
    "    program = lib.make_sort_unique(rasp.tokens, rasp.tokens)\n",
    "  elif program_name == \"hist\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_hist()\n",
    "  elif program_name == \"sort_freq\":\n",
    "    vocab = {\"a\", \"b\", \"c\", \"d\"}\n",
    "    program = lib.make_sort_freq(max_seq_len=max_seq_len)\n",
    "  elif program_name == \"pair_balance\":\n",
    "    vocab = {\"(\", \")\"}\n",
    "    program = lib.make_pair_balance(\n",
    "        sop=rasp.tokens, open_token=\"(\", close_token=\")\")\n",
    "  else:\n",
    "    raise NotImplementedError(f\"Program {program_name} not implemented.\")\n",
    "  return program, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "   Program: sort_unique\n",
      "   Input vocabulary: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n",
      "   Context size: 10\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#@title: Assemble model\n",
    "program_name = \"sort_unique\"  #@param [\"length\", \"frac_prevs\", \"dyck-2\", \"dyck-3\", \"sort\", \"sort_unique\", \"hist\", \"sort_freq\", \"pair_balance\"]\n",
    "max_seq_len = 10 #@param {label: \"Test\", type: \"integer\"}\n",
    "\n",
    "program, vocab = get_program(program_name=program_name,\n",
    "                             max_seq_len=max_seq_len)\n",
    "\n",
    "print(f\"Compiling...\")\n",
    "print(f\"   Program: {program_name}\")\n",
    "print(f\"   Input vocabulary: {vocab}\")\n",
    "print(f\"   Context size: {max_seq_len}\")\n",
    "\n",
    "from tracr.datasets.generated_lib import program_1\n",
    "assembled_model = compiling.compile_rasp_to_model(\n",
    "      program=program,\n",
    "      vocab=vocab,\n",
    "      max_seq_len=max_seq_len,\n",
    "      causal=False,\n",
    "      use_dropout=False, \n",
    "      embedding_size=30,\n",
    "      unembed_at_every_layer=True,\n",
    "      compiler_bos=\"bos\",\n",
    "      compiler_pad=\"pad\",\n",
    "      mlp_exactness=100)\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# dict_keys(['token_embed', 'pos_embed', 'transformer/layer_0/attn/query', 'transformer/layer_0/attn/key', 'transformer/layer_0/attn/value', 'transformer/layer_0/attn/linear', 'transformer/layer_0/mlp/linear_1', 'transformer/layer_0/mlp/linear_2', 'transformer/layer_1/attn/query', 'transformer/layer_1/attn/key', 'transformer/layer_1/attn/value', 'transformer/layer_1/attn/linear', 'transformer/layer_1/mlp/linear_1', 'transformer/layer_1/mlp/linear_2'])\n",
    "# dict_keys(['token_embed', 'pos_embed', 'compressed_transformer/layer_0/attn/query', 'compressed_transformer/layer_0/attn/key', 'compressed_transformer/layer_0/attn/value', 'compressed_transformer/layer_0/attn/linear', 'compressed_transformer/layer_0/mlp/linear_1', 'compressed_transformer/layer_0/mlp/linear_2', 'compressed_transformer/layer_1/attn/query', 'compressed_transformer/layer_1/attn/key', 'compressed_transformer/layer_1/attn/value', 'compressed_transformer/layer_1/attn/linear', 'compressed_transformer/layer_1/mlp/linear_1', 'compressed_transformer/layer_1/mlp/linear_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Forward pass\n",
    "assembled_model.apply([\"bos\", 3, 4, 1, 7, 2, 6, 5, 8, 9, 10]).decoded\n",
    "# assembled_model.apply(['bos', 'a', 'b', 'c', 'x']).decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compressed_transformer': {'w_emb': (30, 45)},\n",
       " 'compressed_transformer/layer_0/attn/key': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/attn/linear': {'b': (45,), 'w': (12, 45)},\n",
       " 'compressed_transformer/layer_0/attn/query': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/attn/value': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_0/mlp/linear_1': {'b': (22,), 'w': (45, 22)},\n",
       " 'compressed_transformer/layer_0/mlp/linear_2': {'b': (45,), 'w': (22, 45)},\n",
       " 'compressed_transformer/layer_1/attn/key': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/attn/linear': {'b': (45,), 'w': (12, 45)},\n",
       " 'compressed_transformer/layer_1/attn/query': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/attn/value': {'b': (12,), 'w': (45, 12)},\n",
       " 'compressed_transformer/layer_1/mlp/linear_1': {'b': (22,), 'w': (45, 22)},\n",
       " 'compressed_transformer/layer_1/mlp/linear_2': {'b': (45,), 'w': (22, 45)},\n",
       " 'pos_embed': {'embeddings': (11, 45)},\n",
       " 'token_embed': {'embeddings': (12, 45)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "@hk.transform\n",
    "def forward_fn(inputs): \n",
    "    compiled_model = assembled_model.get_compiled_model()\n",
    "    return compiled_model(inputs)\n",
    "\n",
    "dummy = jnp.zeros((1, 10), dtype=jnp.int32)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = forward_fn.init(rng, dummy)\n",
    "params.keys()\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
